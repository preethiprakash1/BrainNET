{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":407317,"sourceType":"datasetVersion","datasetId":181273}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries and Packages","metadata":{}},{"cell_type":"code","source":"!pip install -qqq torchmetrics\n!pip install -qqq pytorch-lightning\n!pip install -qqq segmentation-models-pytorch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport glob\n\nimport gc\nimport time\n\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchmetrics\n\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nfrom IPython.display import Image\nfrom skimage import io\n\nfrom pprint import pprint\n\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom IPython.display import display\n\nfrom PIL import Image\n\nimport torchvision\nfrom torchvision import transforms","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"img_data = pd.read_csv('../input/lgg-mri-segmentation/kaggle_3m/data.csv') # mask data?\nimg_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = []\nfor sub_dir_path in glob.glob(\"/kaggle/input/lgg-mri-segmentation/kaggle_3m/\"+\"*\"):\n    try:\n        dir_name = sub_dir_path.split('/')[-1]\n        for filename in os.listdir(sub_dir_path):\n            mask_path = sub_dir_path + '/' + filename\n            data_path.extend([dir_name, mask_path])\n    except Exception as e:\n        print(e)\n\nfilenames = data_path[::2]\nmasks = data_path[1::2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data={\"patient_id\": filenames,\"img_path\": masks})\nprint(df.shape)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_img = df[~df['img_path'].str.contains(\"mask\")]\nmask_img = df[df['img_path'].str.contains(\"mask\")]\noriginal_img, mask_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = sorted(original_img[\"img_path\"].values, key=lambda x : int(x[89:-4]))\nmasks = sorted(mask_img[\"img_path\"].values, key=lambda x : int(x[89:-9]))\n\nidx = random.randint(0, len(imgs)-1)\nprint(\"Image path:\", imgs[idx], \"\\nMask path:\", masks[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mri_df = pd.DataFrame({\"patient_id\": original_img.patient_id.values,\"img_path\": imgs,\n                           'mask_path':masks})\nmri_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_diagnosis(img_path):\n    value = np.max(cv2.imread(img_path))\n    if value > 0 : \n        return 1\n    else:\n        return 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mri_df['mask'] = mri_df['mask_path'].apply(lambda x: get_diagnosis(x))\n\nmri_df['mask_path'] = mri_df['mask_path'].apply(lambda x: str(x))\n\nprint(mri_df.shape)\nmri_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mri_df.drop(columns=['patient_id'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"positive_samples = mri_df[mri_df['mask_path'].apply(get_diagnosis) == 1].sample(n=1183, random_state=42)\n\ntumor_count_before = mri_df['mask_path'].apply(get_diagnosis).sum()\nnon_tumor_count_before = len(mri_df) - tumor_count_before\n\ntransformed_data = []\n\nfor idx, row in positive_samples.iterrows():\n    image_path = row['img_path']\n    mask_path = row['mask_path']\n    \n    image = cv2.imread(image_path)\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n    new_image_path = f\"{idx}.tif\"\n    new_mask_path = f\"{idx}_mask.tif\"\n    \n    scale_factor = random.uniform(0.9, 1.1)\n    image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor)\n    mask = cv2.resize(mask, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_NEAREST)\n    \n    angle = random.uniform(-180, 180)\n    rows, cols, _ = image.shape\n    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n    image = cv2.warpAffine(image, M, (cols, rows))\n    mask = cv2.warpAffine(mask, M, (cols, rows), flags=cv2.INTER_NEAREST)\n    \n    if random.random() < 0.5:\n        image = cv2.transpose(image)\n        mask = cv2.transpose(mask)\n        \n    cv2.imwrite(new_image_path, image)\n    cv2.imwrite(new_mask_path, mask)\n    \n    transformed_data.append({'img_path': new_image_path, 'mask_path': new_mask_path})\n\nmri_df = pd.concat([mri_df, pd.DataFrame(transformed_data)], ignore_index=True)\n\ntumor_count_after = mri_df['mask_path'].apply(get_diagnosis).sum()\nnon_tumor_count_after = len(mri_df) - tumor_count_after\n\nprint(\"Total images with tumors before transformations:\", tumor_count_before)\nprint(\"Total images without tumors before transformations:\", non_tumor_count_before)\nprint(\"Total images with tumors after transformations:\", tumor_count_after)\nprint(\"Total images without tumors after transformations:\", non_tumor_count_after)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"image_transform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n])\n\nmask_transform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_data(img, mask):\n    img = img / 255.\n    mask = mask / 255.\n    mask[mask > 0.5] = 1.0\n    mask[mask <= 0.5] = 0.0\n    \n    return (img, mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df= mri_df, \n                 adjust_data = adjust_data, \n                 image_transform=image_transform, mask_transform=mask_transform):\n        self.df = df\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n        self.adjust_data= adjust_data\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_path = self.df.loc[idx, 'img_path']\n        mask_path = self.df.loc[idx, 'mask_path']\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path)\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n\n        image, mask = self.adjust_data(image, mask)\n\n        if self.image_transform:\n            image = self.image_transform(image).float()\n\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n        return image, mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(df= mri_df,\n                    train_num= int(mri_df.shape[0] * .6), \n                    valid_num= int(mri_df.shape[0] * .8), \n                    bs = 16):\n    \n    train = df[:train_num].reset_index(drop=True)\n    valid = df[train_num : valid_num].reset_index(drop=True)    \n    test  = df[valid_num:].reset_index(drop=True)\n\n    train_ds = MyDataset(df = train)\n    valid_ds = MyDataset(df = valid)\n    test_ds = MyDataset(df = test)\n\n    train_loader = DataLoader(train_ds, batch_size = bs, num_workers = os.cpu_count(), shuffle = True)\n    valid_loader = DataLoader(valid_ds, batch_size = bs, num_workers = os.cpu_count(), shuffle = False)\n    test_loader = DataLoader(test_ds, batch_size = 4, num_workers = os.cpu_count(), shuffle = True)\n    \n    print(\"DataLoader Completed\")\n    \n    return train_loader, valid_loader, test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, valid_loader, test_loader = prepare_loaders(df= mri_df,\n                                                            train_num= int(mri_df.shape[0] * .65), \n                                                            valid_num= int(mri_df.shape[0] * .85), \n                                                            bs = 16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = next(iter(train_loader))\ndata[0].shape, data[1].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention UNet Model","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n\n        # number of input channels is a number of filters in the previous layer\n        # number of output channels is a number of filters in the current layer\n        # \"same\" convolutions\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass UpConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(UpConv, self).__init__()\n\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"Attention block with learnable parameters\"\"\"\n\n    def __init__(self, F_g, F_l, n_coefficients):\n        \"\"\"\n        :param F_g: number of feature maps (channels) in previous layer\n        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n        \"\"\"\n        super(AttentionBlock, self).__init__()\n\n        self.W_gate = nn.Sequential(\n            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(n_coefficients)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(n_coefficients)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, gate, skip_connection):\n        \"\"\"\n        :param gate: gating signal from previous layer\n        :param skip_connection: activation from corresponding encoder layer\n        :return: output activations\n        \"\"\"\n        g1 = self.W_gate(gate)\n        x1 = self.W_x(skip_connection)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = skip_connection * psi\n        return out\n\n\nclass AttentionUNet(nn.Module):\n\n    def __init__(self, img_ch=3, output_ch=1):\n        super(AttentionUNet, self).__init__()\n\n        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = ConvBlock(img_ch, 64)\n        self.Conv2 = ConvBlock(64, 128)\n        self.Conv3 = ConvBlock(128, 256)\n        self.Conv4 = ConvBlock(256, 512)\n        self.Conv5 = ConvBlock(512, 1024)\n\n        self.Up5 = UpConv(1024, 512)\n        self.Att5 = AttentionBlock(F_g=512, F_l=512, n_coefficients=256)\n        self.UpConv5 = ConvBlock(1024, 512)\n\n        self.Up4 = UpConv(512, 256)\n        self.Att4 = AttentionBlock(F_g=256, F_l=256, n_coefficients=128)\n        self.UpConv4 = ConvBlock(512, 256)\n\n        self.Up3 = UpConv(256, 128)\n        self.Att3 = AttentionBlock(F_g=128, F_l=128, n_coefficients=64)\n        self.UpConv3 = ConvBlock(256, 128)\n\n        self.Up2 = UpConv(128, 64)\n        self.Att2 = AttentionBlock(F_g=64, F_l=64, n_coefficients=32)\n        self.UpConv2 = ConvBlock(128, 64)\n\n        self.Conv = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        \"\"\"\n        e : encoder layers\n        d : decoder layers\n        s : skip-connections from encoder layers to decoder layers\n        \"\"\"\n        e1 = self.Conv1(x)\n\n        e2 = self.MaxPool(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.MaxPool(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.MaxPool(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.MaxPool(e4)\n        e5 = self.Conv5(e5)\n\n        d5 = self.Up5(e5)\n\n        s4 = self.Att5(gate=d5, skip_connection=e4)\n        d5 = torch.cat((s4, d5), dim=1) # concatenate attention-weighted skip connection with previous layer output\n        d5 = self.UpConv5(d5)\n\n        d4 = self.Up4(d5)\n        s3 = self.Att4(gate=d4, skip_connection=e3)\n        d4 = torch.cat((s3, d4), dim=1)\n        d4 = self.UpConv4(d4)\n\n        d3 = self.Up3(d4)\n        s2 = self.Att3(gate=d3, skip_connection=e2)\n        d3 = torch.cat((s2, d3), dim=1)\n        d3 = self.UpConv3(d3)\n\n        d2 = self.Up2(d3)\n        s1 = self.Att2(gate=d2, skip_connection=e1)\n        d2 = torch.cat((s1, d2), dim=1)\n        d2 = self.UpConv2(d2)\n\n        out = self.Conv(d2)\n\n        return out\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"def accuracy_function(target, predicted):\n    true_positives = np.sum(predicted * target)\n    true_negatives = np.sum((1 - predicted) * (1 - target))\n    total_pixels = len(predicted)\n    acc = (true_positives + true_negatives) / total_pixels\n    return acc.item()\n\ndef precision_function(target, predicted):\n    true_positives = np.sum(predicted * target)\n    false_positives = np.sum(predicted * (1 - target))\n    precision = true_positives / (true_positives + false_positives + 1e-7)  # Add a small epsilon value to avoid division by zero\n    return precision.item()\n\ndef recall_function(target, predicted):\n    true_positives = np.sum(predicted * target)\n    false_negatives = np.sum((1 - predicted) * target)\n    recall = true_positives / (true_positives + false_negatives + 1e-7)  # Add a small epsilon value to avoid division by zero\n    return recall.item()\n\ndef dice_coeff_binary(y_true, y_pred):\n    \"\"\"Compute Dice coefficient for binary segmentation.\"\"\"\n    eps = 1e-9\n    inter = np.dot(y_pred, y_true)\n    union = np.sum(y_pred) + np.sum(y_true)\n    return ((2 * inter + eps) / (union + eps)).item()\n\ndef specificity_function(y_true, y_pred):\n    \"\"\"Compute specificity for binary classification.\"\"\"\n    true_negatives = np.sum((1 - y_pred) * (1 - y_true))\n    actual_negatives = np.sum(1 - y_true)\n    return true_negatives / actual_negatives\n\ndef intersection_over_union(y_true, y_pred):\n    \"\"\"Compute Intersection over Union for binary segmentation.\"\"\"\n    eps = 1e-9\n    intersection = np.sum(y_pred * y_true)\n    union = np.sum(y_pred) + np.sum(y_true) - intersection\n    iou = (intersection + eps) / (union + eps)\n    return iou.item()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"model = AttentionUNet(3, 1)\nmodel.cuda()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nnum_epochs = 20\ntrain_losses = []\nval_losses = []\ntrain_dices = []\nval_dices = []\n\nbest_val_loss = float('inf') \nfor epoch in range(num_epochs):\n    model.train()\n    total_accuracy = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_dice = 0.0\n    total_specificity = 0.0\n    total_iou = 0.0\n\n    for i, (images, masks) in enumerate(train_loader):\n        images = images.cuda()\n        masks = masks.cuda()\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        threshold = 0.5\n        outputs_thresholded = (outputs > threshold).float()\n\n        outputs_flat = outputs.view(-1)\n        masks_flat = masks.view(-1)\n\n        loss = criterion(outputs_flat, masks_flat)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}, Loss: {loss}\")\n\n        loss.backward()\n        optimizer.step()\n\n        image_index = 0 \n        single_mask = masks[image_index].cpu().detach().numpy()\n        single_output = outputs_thresholded[image_index].cpu().detach().numpy()\n\n\n        outputs_thresholded = outputs_thresholded.cpu().detach().numpy().reshape(-1)\n        masks = masks.cpu().detach().numpy().reshape(-1)\n\n        accuracy = accuracy_function(masks, outputs_thresholded)\n        precision = precision_function(masks, outputs_thresholded)\n        recall = recall_function(masks, outputs_thresholded)\n        dice_score = dice_coeff_binary(masks, outputs_thresholded)\n        specificity = specificity_function(masks, outputs_thresholded)\n        iou = intersection_over_union(masks, outputs_thresholded)\n\n        total_accuracy += accuracy\n        total_precision += precision\n        total_recall += recall\n        total_dice += dice_score\n        total_specificity += specificity\n        total_iou += iou\n\n        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, Dice Score: {dice_score}, Specificity: {specificity}, IOU: {iou}\")\n\n    avg_accuracy = total_accuracy / len(train_loader)\n    avg_precision = total_precision / len(train_loader)\n    avg_recall = total_recall / len(train_loader)\n    avg_dice = total_dice / len(train_loader)\n    avg_specificity = total_specificity / len(train_loader)\n    avg_iou = total_iou / len(train_loader)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss}, Average Accuracy: {avg_accuracy}, Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average Dice: {avg_dice}, Average Specificity: {avg_specificity}, Average IOU: {avg_iou}\")\n\n    train_losses.append(loss)\n    train_dices.append(avg_dice)\n    \n    model.eval()\n    total_val_accuracy = 0.0\n    total_val_precision = 0.0\n    total_val_recall = 0.0\n    total_val_dice = 0.0\n    total_val_specificity = 0.0\n    total_val_loss = 0.0\n    total_val_iou = 0.0\n\n    with torch.no_grad():\n        for i, (images, masks) in enumerate(valid_loader):\n            images = images.cuda()\n            masks = masks.cuda()\n\n            outputs = model(images)\n            threshold = 0.5\n            outputs_thresholded = (outputs > threshold).float()\n\n            outputs_flat = outputs.view(-1)\n            masks_flat = masks.view(-1)\n\n            loss = criterion(outputs_flat, masks_flat)\n            total_val_loss += loss.item()\n\n            outputs_thresholded = outputs_thresholded.cpu().detach().numpy().reshape(-1)\n            masks = masks.cpu().detach().numpy().reshape(-1)\n\n            accuracy = accuracy_function(masks, outputs_thresholded)\n            precision = precision_function(masks, outputs_thresholded)\n            recall = recall_function(masks, outputs_thresholded)\n            dice_score = dice_coeff_binary(masks, outputs_thresholded)\n            specificity = specificity_function(masks, outputs_thresholded)\n            iou = intersection_over_union(masks, outputs_thresholded)\n\n            total_val_accuracy += accuracy\n            total_val_precision += precision\n            total_val_recall += recall\n            total_val_dice += dice_score\n            total_val_specificity += specificity\n            total_val_iou += iou\n\n            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Batch {i+1}/{len(valid_loader)}, Loss: {loss.item()}\")\n\n        avg_val_accuracy = total_val_accuracy / len(valid_loader)\n        avg_val_precision = total_val_precision / len(valid_loader)\n        avg_val_recall = total_val_recall / len(valid_loader)\n        avg_val_dice = total_val_dice / len(valid_loader)\n        avg_val_specificity = total_val_specificity / len(valid_loader)\n        avg_val_loss = total_val_loss / len(valid_loader)\n        avg_val_iou = total_val_iou / len(valid_loader)\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), '/kaggle/working/model_best.ckpt')\n            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss improved. Model saved.\")\n\n        val_losses.append(avg_val_loss)\n        val_dices.append(avg_val_dice)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}, Average Accuracy: {avg_val_accuracy}, Average Precision: {avg_val_precision}, Average Recall: {avg_val_recall}, Average Dice: {avg_val_dice}, Average Specificity: {avg_val_specificity}, Average IOU: {avg_val_iou}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nepochs = range(1, num_epochs + 1)\n\ntrain_losses_np = np.array([loss.cpu().detach().numpy() for loss in train_losses])\nval_losses_np = np.array([loss for loss in val_losses])\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_losses_np, label='Training Loss')\nplt.plot(epochs, val_losses_np, label='Validation Loss')\nplt.title('Training and Validation Loss Curves')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(epochs)\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, num_epochs + 1)\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_dices, label='Training DICE')\nplt.plot(epochs, val_dices, label='Validation DICE')\nplt.title('Training and Validation DICE Scores')\nplt.xlabel('Epochs')\nplt.ylabel('DICE Score')\nplt.xticks(epochs)\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_path = '/kaggle/working/model_best.ckpt'\ncheck_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_state_dict = torch.load(check_path)\n\nmodel.load_state_dict(model_state_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Testing","metadata":{}},{"cell_type":"code","source":"    model.eval()\n    total_test_accuracy = 0.0\n    total_test_precision = 0.0\n    total_test_recall = 0.0\n    total_test_dice = 0.0\n    total_test_specificity = 0.0\n    total_test_loss = 0.0\n    total_test_iou = 0.0\n\n    with torch.no_grad():\n        for i, (images, masks) in enumerate(test_loader):\n            images = images.cuda()\n            masks = masks.cuda()\n\n            outputs = model(images)\n            threshold = 0.5\n            outputs_thresholded = (outputs > threshold).float()\n\n            outputs_flat = outputs.view(-1)\n            masks_flat = masks.view(-1)\n\n            loss = criterion(outputs_flat, masks_flat)\n            total_test_loss += loss.item()\n\n            outputs_thresholded = outputs_thresholded.cpu().detach().numpy().reshape(-1)\n            masks = masks.cpu().detach().numpy().reshape(-1)\n\n            accuracy = accuracy_function(masks, outputs_thresholded)\n            precision = precision_function(masks, outputs_thresholded)\n            recall = recall_function(masks, outputs_thresholded)\n            dice_score = dice_coeff_binary(masks, outputs_thresholded)\n            specificity = specificity_function(masks, outputs_thresholded)\n            iou = intersection_over_union(masks, outputs_thresholded)\n\n            total_test_accuracy += accuracy\n            total_test_precision += precision\n            total_test_recall += recall\n            total_test_dice += dice_score\n            total_test_specificity += specificity\n            total_iou += iou\n\n        avg_test_accuracy = total_test_accuracy / len(test_loader)\n        avg_test_precision = total_test_precision / len(test_loader)\n        avg_test_recall = total_test_recall / len(test_loader)\n        avg_test_dice = total_test_dice / len(test_loader)\n        avg_test_specificity = total_test_specificity / len(test_loader)\n        avg_test_loss = total_test_loss / len(test_loader)\n        avg_test_iou = total_iou / len(test_loader)\n        \n        print(f\"Test Loss: {avg_test_loss}, Average Accuracy: {avg_test_accuracy}, Average Precision: {avg_test_precision}, Average Recall: {avg_test_recall}, Average Dice: {avg_test_dice}, Average Specificity: {avg_test_specificity}, Average IOU: {avg_test_iou}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Segmentations","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmodel.eval()\n\nwith torch.no_grad():\n    for i, (images, masks) in enumerate(test_loader):\n        images = images.cuda()\n        masks = masks.cuda()\n\n        outputs = model(images)\n        threshold = 0.5\n        outputs_thresholded = (outputs > threshold).float()\n\n        for j in range(min(images.shape[0], 50)):  \n            plt.figure(figsize=(15, 5))\n\n            plt.subplot(1, 3, 1)\n            plt.imshow(images[j].cpu().permute(1, 2, 0))\n            plt.title(\"Image\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(masks[j].cpu().squeeze(), cmap='gray')\n            plt.title(\"Ground truth\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(outputs_thresholded[j].cpu().squeeze(), cmap='gray')\n            plt.title(\"Predicted mask\")\n            plt.axis(\"off\")\n\n            plt.show()\n\n        if i * test_loader.batch_size + images.shape[0] >= 50:\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}